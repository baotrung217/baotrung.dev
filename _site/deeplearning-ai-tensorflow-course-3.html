<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

	<!-- block search indexing -->
	<meta name="robots" content="noindex">
	<meta name="googlebot" content="noindex">

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous" async>

  <!-- fontawesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous" async>

  <!-- google fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin async >
  <link href="https://fonts.googleapis.com/css2?family=Comfortaa:wght@700&display=swap" rel="stylesheet" crossorigin async>
  <link href="https://fonts.googleapis.com/css?family=Livvic:400,400i,500,500i,600&display=swap&subset=vietnamese" rel="stylesheet" crossorigin async>
  <link href="https://fonts.googleapis.com/css?family=Lobster&display=swap" rel="stylesheet" crossorigin async>

  <link rel="alternate" hreflang="en" href="/deeplearning-ai-tensorflow-course-3" async />

  <link rel="shortcut icon" type="image/x-icon" href="/img/avatar-icon.ico" async>
  <meta name="author" content="Nguyễn Bảo Trung" />

  <title>
    
      Trung | TF 3 - NLP in TensorFlow
    
  </title>

  

  <!-- feed -->
  <link rel="alternate" type="application/rss+xml" title="Nguyễn Bảo Trung - " href="/feed.xml" async />

  <!-- page (internal) -->
  

  <!-- layout (internal) -->
  
    
      <link rel="stylesheet" href="/css/main.css" async />
    
      <link rel="stylesheet" href="/css/rouge-thi.css" async />
    
  
  

    <!-- Facebook OpenGraph tags -->
  

  
    <meta property="fb:admins" content="baotrung217zzz"/>
  

  
    <meta property="og:title" content="TF 3 - NLP in TensorFlow" />
  

  
    <meta property="og:description" content="Tokernizing + padding Word embeddings IMDB review dataset Sarcasm dataset Pre-tokenized datasets Sequence models RNN idea LSTM idea With vs without LSTM Using a ConvNet IMDB dataset Sequence models and literature This is my note for the 3rd course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by...">
  


  <meta property="og:type" content="website" />

  
    <meta property="og:url" content="http://localhost:4000/deeplearning-ai-tensorflow-course-3" />
    <link rel="canonical" href="http://localhost:4000/deeplearning-ai-tensorflow-course-3" async />
  

  <meta property="og:image" content="http://localhost:4000/img/background.png" />
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="1234">
  <meta property="og:image:height" content="592">

  

  

  

  

</head>


<body>
  <script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '',
      xfbml      : true,
      version    : 'v3.1'
    });
  
    FB.AppEvents.logPageView();
  
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "https://connect.facebook.net/en_US/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>
  <header>
    <nav class="navbar navbar-dark nav-bg navbar-custom fixed-top">
  <div class="container">
    <div class="col-12 col-lg-10 offset-lg-1">
      <div class="nav-container">
        <a class="nav-item" href="http://localhost:4000/">
          <i style="color: #ffeead;" class="fas fa-home" aria-hidden="true"></i>
          <span>Home</span>
        </a>
        <a class="nav-item" href="http://localhost:4000/about">
          <i style="color: #e97c8e;" class="fas fa-fire" aria-hidden="true"></i>
          <span>Me</span>
        </a>
        <a class="nav-item" href="http://localhost:4000/notes">
          <i style="color: #93ceff;" class="fas fa-edit" aria-hidden="true"></i>
          <span>Notes</span>
        </a>
        <div class="nav-search">
          <form action="http://localhost:4000/search" method="get">
            <button class="nav-search-submit" type="submit">
              <i class="fa fa-search" aria-hidden="true"></i>
            </button>
            <input name="q" class="nav-search-input" type="search" placeholder="search notes..." aria-label="search notes...">
          </form>
        </div>
        <a class="nav-item nav-github" href="https://github.com/baotrung217" target="_blank">
          <i class="fab fa-github"></i>
        </a>
    </div>
    </div>
  </div>
</nav>

  </header>

  
  <header class="header">
    <div class="container">
      <div class="row align-items-center justify-content-center">
        <div class="col-md-8 header-content">
          
          
            <div class="icon-photo">
              <img src="http://localhost:4000/img/header/tensorflow.svg">
            </div>
          
          <h1 class="post-title">
            TF 3 - NLP in TensorFlow
          </h1>
          
            <div class="tag-in-post">
              
              
                
                
                
                
                
                  <a href="/tags#coursera">
                    coursera
                  </a>
                

              
                
                
                
                
                
                  <a href="/tags#deeplearning-ai">
                    deeplearning.ai
                  </a>
                

              
                
                
                
                
                
                  <a href="/tags#mooc">
                    mooc
                  </a>
                

              
                
                
                
                
                
                  <a href="/tags#tensorflow">
                    tensorflow
                  </a>
                

              
            </div>
          
          <p>
            
              14 Sep 2020
                        
          </p>
        </div>
      </div>
    </div>
  </header>


<main role="main">
  <section class="section">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-12 col-lg-10 col-xl-9">
          <article  class="page-content ">
            <p style="font-style: italic; color: #777; font-size: 0.95rem; margin-bottom: 2rem;">
  Last modified on 01 May 2022.
</p>




<div id="toc">
  <div class="toc-content">
<ul id="markdown-toc">
  <li><a href="#tokernizing--padding" id="markdown-toc-tokernizing--padding">Tokernizing + padding</a></li>
  <li><a href="#word-embeddings" id="markdown-toc-word-embeddings">Word embeddings</a>    <ul>
      <li><a href="#imdb-review-dataset" id="markdown-toc-imdb-review-dataset">IMDB review dataset</a></li>
      <li><a href="#sarcasm-dataset" id="markdown-toc-sarcasm-dataset">Sarcasm dataset</a></li>
    </ul>
  </li>
  <li><a href="#pre-tokenized-datasets" id="markdown-toc-pre-tokenized-datasets">Pre-tokenized datasets</a></li>
  <li><a href="#sequence-models" id="markdown-toc-sequence-models">Sequence models</a>    <ul>
      <li><a href="#rnn-idea" id="markdown-toc-rnn-idea">RNN idea</a></li>
      <li><a href="#lstm-idea" id="markdown-toc-lstm-idea">LSTM idea</a></li>
      <li><a href="#with-vs-without-lstm" id="markdown-toc-with-vs-without-lstm">With vs without LSTM</a></li>
      <li><a href="#using-a-convnet" id="markdown-toc-using-a-convnet">Using a ConvNet</a></li>
      <li><a href="#imdb-dataset" id="markdown-toc-imdb-dataset">IMDB dataset</a></li>
    </ul>
  </li>
  <li><a href="#sequence-models-and-literature" id="markdown-toc-sequence-models-and-literature">Sequence models and literature</a></li>
</ul>

  </div>
</div>

<p>This is my note for the <a href="https://www.coursera.org/learn/natural-language-processing-tensorflow">3rd course</a> of <a href="https://www.coursera.org/specializations/tensorflow-in-practice">TensorFlow in Practice Specialization</a> given by <a href="http://deeplearning.ai/">deeplearning.ai</a> and taught by Laurence Moroney on Coursera.</p>

<p>👉 Check the codes <a href="https://github.com/dinhanhthi/deeplearning.ai-courses/tree/master/TensorFlow%20in%20Practice">on my Github</a>.<br />
👉 Official <a href="https://github.com/lmoroney/dlaicourse">notebooks</a> on Github.</p>

<p>👉 Go to <a href="/deeplearning-ai-tensorflow-course-1">course 1 - Intro to TensorFlow for AI, ML, DL</a>.<br />
👉 Go to <a href="/deeplearning-ai-tensorflow-course-2">course 2 - CNN in TensorFlow</a>.<br />
👉 Go to <a href="/deeplearning-ai-tensorflow-course-4">course 4 - Sequences, Time Series and Prediction</a>.</p>

<h2 id="tokernizing--padding">Tokernizing + padding</h2>

<p>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-1/notebook_1_tokenizer_basic_examples.html">Tokenizer basic examples.</a> <br />
👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-1/notebook_2_sarcasm_detection.html">Sarcasm detection</a>.</p>

<ul class="noindent">
  <li>A common simple character encoding is ASCII,</li>
  <li>We can encode each word as a number (token) – <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"><code class="language-plaintext highlighter-rouge">Tokenizer</code></a>.</li>
  <li>Tokenize words &gt; build all the words to make a corpus &gt; turn your sentences into lists of values based on these tokens. &gt; manipulate these lists (make the same length, for example)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>

<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'i love my dog'</span><span class="p">,</span>
    <span class="s">'I, love my cat'</span><span class="p">,</span>
    <span class="s">'You love my dog so much!'</span>
<span class="p">]</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">oov_token</span><span class="o">=</span><span class="s">"&lt;OOV&gt;"</span><span class="p">)</span>
            <span class="c1"># num_words: max of words to be tokenized &amp; pick
</span>            <span class="c1">#   the most common 100 words.
</span>            <span class="c1"># More words, more accuracy, more time to train
</span>            <span class="c1"># oov_token: replace unseen words by "&lt;OOV&gt;"
</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span> <span class="c1"># fix texts based on tokens
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># indexing words
</span><span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span>
<span class="k">print</span><span class="p">(</span><span class="n">word_index</span><span class="p">)</span>
<span class="c1"># {'&lt;OOV&gt;': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7, 'so': 8, 'much': 9}
# "!", ",", capital, ... are removed
</span></code></pre></div></div>

<p>👉 <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer">tf.keras.preprocessing.text.Tokenizer</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># encode sentences
</span><span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span>
<span class="c1"># [[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 5, 8, 9]]
# if a word is not in the word index, it will be lost in the text_to_sequences()
</span></code></pre></div></div>

<p>👉 <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences">tf.keras.preprocessing.sequence.pad_sequences</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># make encoded sentences equal
</span><span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>

<span class="n">padded</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">maxlen</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">"post"</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="s">"post"</span><span class="p">)</span>
         <span class="c1"># maxlen: max len of encoded sentence
</span>         <span class="c1"># value: value to be filld (default 0)
</span>         <span class="c1"># padding: add missing values at beginning or ending of sentence?
</span>         <span class="c1"># truncating: longer than maxlen? cut at beginning or ending?
</span><span class="k">print</span><span class="p">(</span><span class="n">padded</span><span class="p">)</span>
<span class="c1"># [[ 4  2  3  5 -1]
#  [ 4  2  3  6 -1]
#  [ 7  2  3  5  8]]
</span></code></pre></div></div>

<p>👉 <a href="https://rishabhmisra.github.io/publications/">Sarcasm detection dataset.</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># read json text
</span><span class="kn">import</span> <span class="nn">json</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"/tmp/sarcasm.json"</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">datastore</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">sentences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">urls</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">datastore</span><span class="p">:</span>
    <span class="n">sentences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s">'headline'</span><span class="p">])</span>
    <span class="n">labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s">'is_sarcastic'</span><span class="p">])</span>
    <span class="n">urls</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s">'article_link'</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="word-embeddings">Word embeddings</h2>

<p>👉 <a href="https://projector.tensorflow.org/">Embedding projector - visualization of high-dimensional data</a><br />
👉 <a href="http://ai.stanford.edu/~amaas/data/sentiment/">Large Movie Review Dataset</a></p>

<h3 id="imdb-review-dataset">IMDB review dataset</h3>

<p>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-2/notebook_1_IMDB_reviews.html">Train IMDB review dataset</a>. <br />
👉 <a href="https://www.coursera.org/lecture/natural-language-processing-tensorflow/notebook-for-lesson-1-Q1Ln5">Video explain the code</a>.</p>

<ul class="noindent">
  <li><strong>Word embeddings</strong> = the idea in which words and associated words are <em>clustered as vectors</em> in a multi-dimensional space. That allows words with similar meaning to have a similar representation.</li>
  <li>The meaning of the words can come from labeling of the dataset.
    <ul>
      <li><em>Example</em>: “dull” and “boring” show up a lot in negative reviews =&gt; they have similar sentiments =&gt; they are close to each other in the sentence =&gt; thus their vector will be similar =&gt; NN train + learn these vectors + associating them with the labels to come up with what’s called in embedding.</li>
    </ul>
  </li>
  <li>The purpose of <em>embedding dimension</em> is the number of dimensions for the vector representing the word encoding.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="k">print</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">__version__</span><span class="p">)</span> <span class="c1"># check version of tensorflow
</span>
<span class="c1"># If you are using tf1, you need below code
</span><span class="n">tf</span><span class="p">.</span><span class="n">enable_eager_execution</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># IMDB reviews dataset
</span><span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="n">tfds</span>
<span class="n">imdb</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">tfds</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"imdb_reviews"</span><span class="p">,</span> <span class="n">with_info</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">as_supervised</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">imdb</span><span class="p">[</span><span class="s">'train'</span><span class="p">],</span> <span class="n">imdb</span><span class="p">[</span><span class="s">'test'</span><span class="p">]</span>

<span class="k">for</span> <span class="n">s</span><span class="p">,</span><span class="n">l</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span> <span class="c1"># "s" for sentences "l" for labels
</span>    <span class="c1"># The values for "s" and "l" are tensors
</span>    <span class="c1"># so we need to extracr their values
</span>    <span class="n">training_sentences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">numpy</span><span class="p">().</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf8'</span><span class="p">))</span>
    <span class="n">training_labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Prepare for the NN
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">16</span> <span class="c1"># embedding to dim 16
</span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">120</span> <span class="c1"># of each sentence
</span><span class="n">trunc_type</span><span class="o">=</span><span class="s">'post'</span> <span class="c1"># cut the last words
</span><span class="n">oov_tok</span> <span class="o">=</span> <span class="s">"&lt;OOV&gt;"</span> <span class="c1"># replace not-encoded words by this
</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">oov_token</span><span class="o">=</span><span class="n">oov_tok</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">training_sentences</span><span class="p">)</span>
    <span class="c1"># encoding the words
</span><span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span>
    <span class="c1"># list of word index (built based on training set)
</span>    <span class="c1"># there may be many oov_tok in test set
</span><span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">training_sentences</span><span class="p">)</span>
    <span class="c1"># apply on sentences
</span><span class="n">padded</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span><span class="n">maxlen</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="n">trunc_type</span><span class="p">)</span>
    <span class="c1"># padding the sentences
</span>
<span class="c1"># apply to the test set
</span><span class="n">testing_sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">testing_sentences</span><span class="p">)</span>
<span class="n">testing_padded</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">testing_sequences</span><span class="p">,</span><span class="n">maxlen</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simple NN
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">),</span>
                              <span class="c1"># The result of embedding will be a 2D array:
</span>                              <span class="c1"># length of sentence x embedding_dim
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="c1"># Alternatively (a little diff on speed and accuracy):
</span>    <span class="c1"># tf.keras.layers.GlobalAveragePooling1D()
</span>    <span class="c1">#   average across the vectors to flatten it out
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training
</span><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">padded</span><span class="p">,</span> <span class="n">training_labels_final</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">testing_padded</span><span class="p">,</span> <span class="n">testing_labels_final</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># the result
</span><span class="n">e</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># get the result of the embedding layers
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">e</span><span class="p">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># shape: (vocab_size, embedding_dim)
</span></code></pre></div></div>

<p>If you wanna visualize the result (in 3D) with <a href="https://projector.tensorflow.org/">Embedding projector</a>,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">io</span>

<span class="n">out_v</span> <span class="o">=</span> <span class="n">io</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'vecs.tsv'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf-8'</span><span class="p">)</span>
<span class="n">out_m</span> <span class="o">=</span> <span class="n">io</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'meta.tsv'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf-8'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
  <span class="n">word</span> <span class="o">=</span> <span class="n">reverse_word_index</span><span class="p">[</span><span class="n">word_num</span><span class="p">]</span>
  <span class="n">embeddings</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">word_num</span><span class="p">]</span>
  <span class="n">out_m</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">word</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
  <span class="n">out_v</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="p">])</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
<span class="n">out_v</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">out_m</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>

<span class="k">try</span><span class="p">:</span>
  <span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">files</span>
<span class="k">except</span> <span class="nb">ImportError</span><span class="p">:</span>
  <span class="k">pass</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">files</span><span class="p">.</span><span class="n">download</span><span class="p">(</span><span class="s">'vecs.tsv'</span><span class="p">)</span>
  <span class="n">files</span><span class="p">.</span><span class="n">download</span><span class="p">(</span><span class="s">'meta.tsv'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="sarcasm-dataset">Sarcasm dataset</h3>

<p>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-2/notebook_2_sacarsm.html">Train Sacarsm dataset</a>. <br /></p>

<ul>
  <li>In text data, it usually happens that the accuracy increase over the number of training but the loss increase sharply also. We can “play” with hyperparameter to see the effect.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Run this to ensure TensorFlow 2.x is used
</span><span class="k">try</span><span class="p">:</span>
  <span class="c1"># %tensorflow_version only exists in Colab.
</span>  <span class="o">%</span><span class="n">tensorflow_version</span> <span class="mf">2.</span><span class="n">x</span>
<span class="k">except</span> <span class="nb">Exception</span><span class="p">:</span>
  <span class="k">pass</span>
</code></pre></div></div>

<h2 id="pre-tokenized-datasets">Pre-tokenized datasets</h2>

<p>👉 <a href="https://github.com/tensorflow/datasets/blob/master/docs/catalog/imdb_reviews.md">datasets/imdb_reviews.md at master · tensorflow/datasets</a><br />
👉 <a href="https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder">tfds.features.text.SubwordTextEncoder  |  TensorFlow Datasets</a><br />
👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-2/notebook_3_pre-tokenizer.html">Pre-tokenizer example</a>. <br />
👉 <a href="https://www.coursera.org/lecture/natural-language-processing-tensorflow/notebook-for-lesson-3-piQXt">Video exaplain the codes</a>.</p>

<ul>
  <li>There are someones who did the work (tokenization) for you.</li>
  <li>Try on IMDB dataset that has been pre-tokenized.</li>
  <li>The tokenization is done on <strong>subwords</strong>!</li>
  <li>The sequence of words can be just important as their existence.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load imdb dataset from tensorflow
</span><span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="n">tfds</span>
<span class="n">imdb</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">tfds</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"imdb_reviews/subwords8k"</span><span class="p">,</span> <span class="n">with_info</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">as_supervised</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># extract train/test sets
</span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">imdb</span><span class="p">[</span><span class="s">'train'</span><span class="p">],</span> <span class="n">imdb</span><span class="p">[</span><span class="s">'test'</span><span class="p">]</span>

<span class="c1"># take the tokernizer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">info</span><span class="p">.</span><span class="n">features</span><span class="p">[</span><span class="s">'text'</span><span class="p">].</span><span class="n">encoder</span>

<span class="k">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">subwords</span><span class="p">)</span>
<span class="c1"># ['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_',...
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample_string</span> <span class="o">=</span> <span class="s">'TensorFlow, from basics to mastery'</span>

<span class="n">tokenized_string</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sample_string</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">'Tokenized string is {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">))</span>
<span class="c1"># Tokenized string is [6307, 2327, 4043, 2120, 2, 48, 4249, 4429, 7, 2652, 8050]
</span>
<span class="n">original_string</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">'The original string: {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">original_string</span><span class="p">))</span>
<span class="c1"># The original string: TensorFlow, from basics to mastery
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># take a look on tokenized string
# case sensitive + punctuation maintained
</span><span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">tokenized_string</span><span class="p">:</span>
  <span class="k">print</span> <span class="p">(</span><span class="s">'{} ----&gt; {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">([</span><span class="n">ts</span><span class="p">])))</span>

<span class="c1"># 6307 ----&gt; Ten
# 2327 ----&gt; sor
# 4043 ----&gt; Fl
# ...
</span></code></pre></div></div>

<ul>
  <li>The code run quite long (4 minutes each epoch if using GPU on colab) because there are a lot of hyperparameters and sub-words.</li>
  <li>Result: 50% acc &amp; loss is decreasing but very small.
    <ul>
      <li>Because we are using sub-words, not for-words -&gt; they (sub-words) are nonsensical. -&gt; they are only when we put them together in sequences -&gt; <strong>learning from sequences would be a great way forward</strong> -&gt; <strong>RNN</strong> (Recurrent Neural Networks)</li>
    </ul>
  </li>
</ul>

<h2 id="sequence-models">Sequence models</h2>

<ul>
  <li>The relative ordering, the sequence of words, matters for the meaning of the sentence .</li>
  <li>For NN to take into account for the <strong>ordering of the words</strong>: <strong>RNN</strong> (Recurrent Neural Networks), <strong>LSTM</strong> (Long short-term memory).</li>
  <li><strong>Why not RNN but LSTM ?</strong> With RNN, the context is preserved from timstamp to timestamp BUT that may get lost in longer sentences =&gt; LSTM gets better because it has cell state.</li>
  <li><strong>Example of using LSTM</strong>: “<em>I grew up in Ireland, I went to school and at school, they made me learn how to speak…</em>” =&gt; “speak” is the context and we go back to the beginning to catch “Ireland”, then the next word could be “leanr how to speak <strong>Gaelic</strong>”!</li>
</ul>

<h3 id="rnn-idea">RNN idea</h3>

<p>👉 <a href="/deeplearning-ai-course-5">Note of the course of sequence model</a>.</p>

<ul class="noindent">
  <li>The usual NN, something like “f(data, labels)=rules” cannot take into account of sequences.</li>
  <li><strong>An example of using sequences</strong>: Fibonacci sequence =&gt; the result of current function is the input of next function itself,…</li>
</ul>

<p class="img-70 pop"><img src="/img/post/mooc/tf/rnn-basic-idea.png" alt="RNN basic idea" />
<em>RNN basic idea (<a href="https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359">source</a>).</em></p>

<h3 id="lstm-idea">LSTM idea</h3>

<p>👉 (Video) <a href="https://www.youtube.com/watch?v=8HyCNIVRbSU&amp;feature=emb_title">Illustrated Guide to LSTM’s and GRU’s: A step by step explanation</a> &amp; <a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">its article</a>. <br /></p>

<ul class="noindent">
  <li>Sometimes, the sequence context leads to lose information like the example of “Ireland” and “Gaelic” before.</li>
  <li>LSTM has an additional pipeline called <strong>Cell State</strong>. It can pass through the network to impact it + help to keep context from earlier tokens relevance.</li>
</ul>

<p class="img-75 pop"><img src="/img/post/mooc/tf/lstm-basic-idea.png" alt="LSTM basic idea" />
<em>LSTM basic idea (image from the course).</em></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># SINGLE LAYER LSTM
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">)),</span>
      <span class="c1"># 64: #oututs desired (but the result may be different)
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<p>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_1_IMDB_subwords_8K_with_single_layer_LSTM.html">IMDB Subwords 8K with Single Layer LSTM</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># MULTI PLAYER LSTM
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">)),</span>
      <span class="c1"># return_sequences=True: required if we wanna feed LSTM into another one
</span>      <span class="c1"># It ensures that the output of LSTM match the desired inputs of the next one
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">)),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<p>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_2_IMDB_subwords_8K_with_multi_layer_LSTM.html">IMDB Subwords 8K with Multi Layer LSTM</a></p>

<p class="img-90 pop"><img src="/img/post/mooc/tf/1layer-vs-2layer-lstm.png" alt="1layer vs 2 later LSTM acc" />
<em>1 layer vs 2 layer LSTM accuracy after 50 epochs (image from the course). 2 layer is better (smoother) which makes us more confident about the model. The validation acc is sticked to 80% because we used 8000 sub-words taken from training set, so there may be many tokens from the test set that would be out of vocabulary.</em></p>

<h3 id="with-vs-without-lstm">With vs without LSTM</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># WITHOUT LSTM (like previous section)
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span>
                              <span class="n">input_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">),</span>
    <span class="c1">#
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">GlobalmaxPooling1D</span><span class="p">(),</span>
    <span class="c1">#
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># WITH LSTM
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span>
                              <span class="n">input_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">),</span>
    <span class="c1">#
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">)),</span>
    <span class="c1">#
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<p class="img-90 pop"><img src="/img/post/mooc/tf/with-vs-without-lstm.png" alt="With vs without LSTM" />
<em>With vs without LSTM (image from the course). With LSTM is really better but there is still overfitting here.</em></p>

<h3 id="using-a-convnet">Using a ConvNet</h3>

<p>👉 <a href="https://www.coursera.org/lecture/natural-language-processing-tensorflow/using-a-convolutional-network-fSE8o">Video explains the dimension</a>.<br />
👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_3_IMDB_subwords_8K_with_Conv.html">IMDB Subwords 8K with 1D Convolutional Layer</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
    <span class="c1">#
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
    <span class="c1">#
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">GlobalAveragePooling1D</span><span class="p">(),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<p class="img-90 pop"><img src="/img/post/mooc/tf/using-conv-net.png" alt="Using Convolution network." />
<em>Using Convolution network. (image from the course). It’s really better but there is overfitting there.</em></p>

<h3 id="imdb-dataset">IMDB dataset</h3>

<p>👉 Notebook: <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_4_IMDB_review_with_GRU.html">IMDB Reviews with GRU (and optional LSTM and Conv1D)</a>. <br />
👉 <a href="https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/NFvFd/going-back-to-the-imdb-dataset">Video compares the results</a>.</p>

<p>Try with 3 different choices:</p>

<ul>
  <li><strong>Simple NN</strong>: 5s/epoch, 170K params, nice acc, overfitting.</li>
  <li><strong>LSTM</strong>: 43s/epoch, 30K params, acc better, overfitting.</li>
  <li><strong>GRU</strong> (Gated Recurrent Unit layer, a different type of RNN): 20s/epoch, 169K params, very good acc, overfitting.</li>
  <li><strong>Conv1D</strong>: 6s/epoch, 171K params, good acc, overfitting.</li>
</ul>

<p><strong>Remark</strong>: <mark>With the texts, you'll probably get a bit more overfitting than you would have done with images.</mark> Because we have out of voca words in validation data.</p>

<h2 id="sequence-models-and-literature">Sequence models and literature</h2>

<p>One application of sequence models: read text then <strong>generate another look-alike text</strong>.</p>

<p>👉 <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-4/notebook_1_find_the_next_word_trained_from_a_song.html">Notebook 1</a> &amp; <a href="https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/B80b0/notebook-for-lesson-1">explaining video</a>.</p>

<ul>
  <li>How they predict a new word in the notebook? -&gt; Check <a href="https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/LGBS2/predicting-a-word">this video</a>.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_sequences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
	<span class="c1"># convert each sentence to list of numbers
</span>	<span class="n">token_list</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">line</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
	<span class="c1"># convert each list to n-gram sequence
</span>	<span class="c1"># eg. from [1,2,3,4,5]
</span>	<span class="c1"># 		to [1,2], [1,2,3], [1,2,3,4], [1,2,3,4,5]
</span>	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_list</span><span class="p">)):</span>
		<span class="n">n_gram_sequence</span> <span class="o">=</span> <span class="n">token_list</span><span class="p">[:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
		<span class="n">input_sequences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_gram_sequence</span><span class="p">)</span>

<span class="c1"># pad sequences to the maximum length of all sentences
</span><span class="n">max_sequence_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_sequences</span><span class="p">])</span>
<span class="n">input_sequences</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">pad_sequences</span><span class="p">(</span><span class="n">input_sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_sequence_len</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'pre'</span><span class="p">))</span>

<span class="c1"># create predictors and label
# [0,0,1,2] -&gt; 2 is label
# [0,1,2,3] -&gt; 3 is label
# [1,2,3,4] -&gt; 4 is label
</span><span class="n">xs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">input_sequences</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">input_sequences</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># one-hot encoding the labels (classification problem)
</span><span class="n">ys</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">total_words</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">total_words</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">max_sequence_len</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">20</span><span class="p">)))</span> <span class="c1"># take only 20 units (bi-direction) to train
</span><span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">total_words</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">seed_text</span> <span class="o">=</span> <span class="s">"Laurence went to dublin"</span>
<span class="n">next_words</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">next_words</span><span class="p">):</span>
	<span class="n">token_list</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">seed_text</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
	<span class="c1"># "went to dublin" -&gt; [134, 13, 59]
</span>	<span class="n">token_list</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">([</span><span class="n">token_list</span><span class="p">],</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_sequence_len</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'pre'</span><span class="p">)</span>
	<span class="c1">#  [0, 0, 0, 0, 0, 0, 0, 134, 13, 59]
</span>	<span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict_classes</span><span class="p">(</span><span class="n">token_list</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
	<span class="n">output_word</span> <span class="o">=</span> <span class="s">""</span>
	<span class="c1"># revert an index back to the word
</span>	<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
		<span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">predicted</span><span class="p">:</span>
			<span class="n">output_word</span> <span class="o">=</span> <span class="n">word</span>
			<span class="k">break</span>
	<span class="c1"># add predicted word to the seed text and make another prediction
</span>	<span class="n">seed_text</span> <span class="o">+=</span> <span class="s">" "</span> <span class="o">+</span> <span class="n">output_word</span>
<span class="k">print</span><span class="p">(</span><span class="n">seed_text</span><span class="p">)</span>
<span class="c1"># all the words are predicted based on the probability
# next one will be less certain than the previous
# -&gt; less meaningful
</span></code></pre></div></div>

<ul>
  <li>Using more words will help.</li>
</ul>

<p>👉 <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-4/notebook_3_more_data_on_train.html">Notebook 3 (more data)</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># read from a file
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">'/tmp/irish-lyrics-eof.txt'</span><span class="p">).</span><span class="n">read</span><span class="p">()</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">lower</span><span class="p">().</span><span class="n">split</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>A little changes from the previous,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">total_words</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">max_sequence_len</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">150</span><span class="p">)))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">total_words</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
<span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="c1"># customized optimizer
</span><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">adam</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="c1">#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Different convernges can create different poetry.</li>
  <li>If we use one-hot for a very big corpus -&gt; take a lot of RAM -&gt; use <strong>character-based prediction</strong> -&gt; #unique characters is far less than #unique words. -&gt; <a href="https://www.tensorflow.org/tutorials/text/text_generation">notebook “Text generation with RNN”</a></li>
</ul>

<p>👉 Notebook <a href="https://dinhanhthi.com/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-4/notebook_4_using_lstm_write_shakespeare.html">Using LSTMs, see if you can write Shakespeare!</a></p>


<!-- <div class="box-error">
<h4>Notice an error?</h4>
Everything on this site is <a href="https://github.com/baotrung217/dinhanhthi.com" target="_blank">published</a> on Github. If you find something wrong, <a href="https://github.com/dinhanhthi/dinhanhthi.com/edit/master/_posts/mooc/2020-09-14-deeplearning-ai-tensorflow-course-3.md" target="_blank">just edit it</a> or <a href="mailto:baotrung217@gmail.com?subject=Suggest to edit post 'TF 3 - NLP in TensorFlow'">tell me</a> about it.
</div> -->

<div class="sec-comment">

  

  <button class="btn btn-info w-100" id="show-comments" onclick="load_comment();return false;">
    If you find something wrong or need a comment, click here.
  </button>

  <div id='comment-box' style="display: none;">
    <script src="https://utteranc.es/client.js"
            repo="dinhanhthi/dinhanhthi.com"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>
  </div>

  <script>
    var comment_loaded = false;
    function load_comment() {
        if (!comment_loaded)  {
            comment_loaded = true;
            document.getElementById("show-comments").style.display = "none";
            document.getElementById("comment-box").style.display = "block";
        }
    }
  </script>
</div>

<div class="modal fade" id="imagemodal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-dialog-centered modal-lg modal-xl">
    <div class="modal-content">
      <div class="modal-body">
        <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
        <img src="" class="imagepreview" style="width: 100%;" >
      </div>
    </div>
  </div>
</div>



          </article >
        </div>
      </div>
    </div>
  </section>
</main>



  <footer class="footer">
    <div class="container">
  <div class="row justify-content-center">
    <div class="col-12 col-md-8 col-lg-6 text-center p-3">
      <div class="footer-info">
        <a href="https://baotrung.dev" target="_blank">Trung</a>
        &nbsp;&copy;&nbsp;
        2022
        &nbsp;&bull;&nbsp;
        <a href="http://localhost:4000/about-the-notes">
          About the notes
        </a>
        &nbsp;&bull;&nbsp;
        <a href="https://pobo.dinhanhthi.com" target="_blank">
          Po Bo
        </a>
        &nbsp;&bull;&nbsp;
        <a href="/for-me-only">
          For me only
        </a>
        &nbsp;&bull;&nbsp;
        <a class="donate" href="http://localhost:4000/donate">
          Support Trung
        </a>
      </div>
    </div>
  </div>
</div>

  </footer>

  





  

  <script src="https://code.jquery.com/jquery-1.10.1.min.js" integrity="sha256-SDf34fFWX/ZnUozXXEH0AeB+Ip3hvRsjLwp6QNTEb3k=" crossorigin="anonymous" async></script>

  <!-- scrolling-toc -->
  <script src="http://localhost:4000/js/scrolling-toc.js" type="text/javascript" async></script>

  <!-- bootstrap -->
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous" async></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous" async></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous" async></script>

  <script src="http://localhost:4000/js/click_to_zoom.js" async></script>

  <script src="http://localhost:4000/js/anchor.js" async></script>

</body>
</html>
